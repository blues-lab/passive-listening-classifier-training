{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-credit",
   "metadata": {},
   "source": [
    "# Infersent based Simalirity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dated-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-spanish",
   "metadata": {},
   "source": [
    "### Note this does require downloading GloVe to some folder\n",
    "Download Glove via\n",
    "```sh\n",
    "mkdir GloVe\n",
    "curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
    "```\n",
    "and the encoders\n",
    "```sh\n",
    "mkdir encoder\n",
    "curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
    "```\n",
    "^Modify the Above for your operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advance-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samples.txt\") as f:\n",
    "    sentences = f.readlines()\n",
    "with open(\"triggers.txt\") as f:\n",
    "    triggers = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manual-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own path\n",
    "W2V_PATH = f\"GloVe/glove.840B.300d.txt\"\n",
    "model_version = 1\n",
    "MODEL_PATH = f'encoder/infersent%s.pkl' % model_version\n",
    "\n",
    "def generate_Infersent_model():\n",
    "    # Load model\n",
    "    params_model = {'bsize': 64, \n",
    "                    'word_emb_dim': 300, \n",
    "                    'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max',\n",
    "                    'dpout_model': 0.0, \n",
    "                    'version': model_version}\n",
    "    model = InferSent(params_model)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "    model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "    model.build_vocab_k_words(K=100000)\n",
    "    model.update_vocab(sentences)\n",
    "    return model\n",
    "\n",
    "def get_doc2vec(text, model, verbose=False):\n",
    "    emb = model.encode(text, verbose=verbose)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "northern-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Found 819(/1017) words with w2v vectors\n",
      "New vocab size : 100822 (added 819 words)\n"
     ]
    }
   ],
   "source": [
    "model = generate_Infersent_model()\n",
    "\n",
    "# Run the following if saved\n",
    "# PATH = (os.path.dirname(os.path.abspath(__file__))) + \"/\"\n",
    "# model = torch.load(PATH + \"infraset_model.torch\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capable-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-softball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "treated-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import compute_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing false positives:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikranth/Documents/Software/jobs/blues/passive-listening-classifier-training/infersent_consine_simalirity/models.py:230: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sentences = np.array(sentences)[idx_sort]\n"
     ]
    }
   ],
   "source": [
    "def is_infersent_trigger(sent1, sent2):\n",
    "    res = get_doc2vec([sent1, sent2], model)\n",
    "    return cosine(res[0], res[1]) > .7\n",
    "    \n",
    "compute_train_test_split(is_infersent_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "twelve-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:36<00:00,  2.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37, 1.2333333333333334)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(triggers, triggers, is_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "operating-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_doc2vec([\"The beaches are very dirty in Mumbai\", \"What will be the weather be?\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "average-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.394147"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "regulation-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_doc2vec([\"is the weather going to get hotter?\", \"What will be the weather be?\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "colonial-basis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77164006"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "infrared-updating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9815"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "historic-wrist",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-f9ea1b8cc5d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/berkeley/venv/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, disable, exclude, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/berkeley/venv/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-citizenship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
