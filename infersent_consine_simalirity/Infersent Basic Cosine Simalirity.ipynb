{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-credit",
   "metadata": {},
   "source": [
    "# Infersent based Simalirity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dated-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-spanish",
   "metadata": {},
   "source": [
    "### Note this does require downloading GloVe to some folder\n",
    "Download Glove via\n",
    "```sh\n",
    "mkdir GloVe\n",
    "curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "unzip GloVe/glove.840B.300d.zip -d GloVe/\n",
    "```\n",
    "and the encoders\n",
    "```sh\n",
    "mkdir encoder\n",
    "curl -Lo encoder/infersent1.pkl https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
    "```\n",
    "^Modify the Above for your operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advance-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samples.txt\") as f:\n",
    "    sentences = f.readlines()\n",
    "with open(\"triggers.txt\") as f:\n",
    "    triggers = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manual-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own path\n",
    "W2V_PATH = f\"GloVe/glove.840B.300d.txt\"\n",
    "model_version = 1\n",
    "MODEL_PATH = f'encoder/infersent%s.pkl' % model_version\n",
    "\n",
    "def generate_Infersent_model():\n",
    "    # Load model\n",
    "    params_model = {'bsize': 64, \n",
    "                    'word_emb_dim': 300, \n",
    "                    'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max',\n",
    "                    'dpout_model': 0.0, \n",
    "                    'version': model_version}\n",
    "    model = InferSent(params_model)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "    model.set_w2v_path(W2V_PATH)\n",
    "\n",
    "    model.build_vocab_k_words(K=100000)\n",
    "    model.update_vocab(sentences)\n",
    "    return model\n",
    "\n",
    "def get_doc2vec(text, model, verbose=False):\n",
    "    emb = model.encode(text, verbose=verbose)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "northern-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size : 100000\n",
      "Found 819(/1017) words with w2v vectors\n",
      "New vocab size : 100822 (added 819 words)\n"
     ]
    }
   ],
   "source": [
    "model = generate_Infersent_model()\n",
    "\n",
    "# Run the following if saved\n",
    "# PATH = (os.path.dirname(os.path.abspath(__file__))) + \"/\"\n",
    "# model = torch.load(PATH + \"infraset_model.torch\")\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "capable-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-softball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "treated-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import compute_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qualified-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing false positives:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikranth/Documents/Software/jobs/blues/passive-listening-classifier-training/infersent_consine_simalirity/models.py:230: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  sentences = np.array(sentences)[idx_sort]\n",
      "100%|██████████| 50/50 [11:24<00:00, 13.70s/it]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0)\n",
      "Compute true positives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:28<00:00,  2.05s/it]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 0.9285714285714286)\n",
      "Compute neg distractors rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:37<00:00,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 0.6111111111111112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def is_infersent_trigger(sent1, sent2):\n",
    "    res = get_doc2vec([sent1, sent2], model)\n",
    "    return cosine(res[0], res[1]) > .7\n",
    "\n",
    "compute_train_test_split(is_infersent_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "twelve-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def sample_trigger(text1, text2):\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    return doc1.similarity(doc2) > .7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aerial-newspaper",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a3511e54b557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import the Universal Sentence Encoder's TF Hub module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msimilarity_input_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msimilarity_message_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_input_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(similarity_input_placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "operating-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_doc2vec([\"The beaches are very dirty in Mumbai\", \"What will be the weather be?\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-grill",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing false positives:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:51<00:00,  3.43s/it]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0)\n",
      "Compute true positives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.25it/s]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 0.8571428571428571)\n",
      "Compute neg distractors rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [00:07<00:39,  2.61s/it]"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "def is_trigger_sentence_transformer(sent1, sent2):\n",
    "#Compute embedding for both lists\n",
    "    embeddings1 = model.encode([sent1], convert_to_tensor=True)\n",
    "    embeddings2 = model.encode([sent2], convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    return cosine_scores[0] > .7\n",
    "\n",
    "compute_train_test_split(is_trigger_sentence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "solved-attachment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.24G/1.24G [01:45<00:00, 11.7MB/s]  \n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing false positives:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [13:39<00:00, 16.40s/it]\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.02)\n",
      "Compute true positives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:45<00:00,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 0.7142857142857143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('stsb-bert-large')\n",
    "compute_train_test_split(is_trigger_sentence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-andrew",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def sample_trigger(text1, text2):\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    return doc1.similarity(doc2) > .7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "colonial-basis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement import\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for import\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/home/vikranth/Documents/berkeley/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-updating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-wrist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-citizenship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
